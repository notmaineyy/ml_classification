# -*- coding: utf-8 -*-
"""trainingModel.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1KlUKSc_1ux7-QVcldBXfmViMJhBJZYSj

# Training Portion
https://www.tensorflow.org/tutorials/images/classification

### Splitting of data into training and validation sets
"""

import matplotlib.pyplot as plt
import numpy as np
import PIL
import tensorflow as tf

from tensorflow import keras
from tensorflow.keras import layers
from tensorflow.keras.models import Sequential

batch_size = 32
img_height = 100 #960
img_width = 350
#1350
data_dir = '/content/drive/MyDrive/2023_internship/dataset_20200803/onlyAugmentedFlir/flipped'

# Load the dataset and perform class-wise splitting
dataset = tf.keras.utils.image_dataset_from_directory(
    data_dir,
    validation_split=0.2,
    subset="training",
    seed=123,
    image_size=(img_height, img_width),
    batch_size=batch_size)

class_names = dataset.class_names
num_classes = len(class_names)

# Create balanced validation and testing datasets
val_ds = tf.keras.utils.image_dataset_from_directory(
    data_dir,
    validation_split=0.2,
    subset="validation",
    seed=123,
    image_size=(img_height, img_width),
    batch_size=batch_size)

"""### Only needed if dataset is too huge, and need to train model with half of the dataset first"""

# Calculate the number of samples in the training dataset
num_samples = dataset.cardinality().numpy()

# Split the dataset into two parts
split_index = num_samples // 2

# Create two separate datasets for the first and second parts
train_ds_part1 = dataset.take(split_index)
train_ds_part2 = dataset.skip(split_index)

"""### Preparing of Model"""

# just to see if there are any extra folders
print(class_names)

ship_categories = class_names

ship_dict = {index: category for index, category in enumerate(class_names)}

# print(ship_dict)

AUTOTUNE = tf.data.AUTOTUNE

dataset = dataset.cache().shuffle(1000).prefetch(buffer_size=AUTOTUNE)
val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)

normalization_layer = layers.Rescaling(1./255)

model_1 = Sequential([
  layers.Rescaling(1./255, input_shape=(img_height, img_width, 3)),
  layers.Conv2D(16, 3, padding='same', activation='relu'),
  layers.MaxPooling2D(),
  layers.Conv2D(32, 3, padding='same', activation='relu'),
  layers.MaxPooling2D(),
  layers.Conv2D(64, 3, padding='same', activation='relu'),
  layers.MaxPooling2D(),
  layers.Flatten(),
  layers.Dense(128, activation='relu'),
  layers.Dense(num_classes)
])

model_1.compile(optimizer='adam',
              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
              metrics=['accuracy'])

model_1.summary()

"""### For transfer learning purposes
(training existing models)
"""

from tensorflow.keras.models import load_model
model_1 = load_model('/content/drive/MyDrive/flir_model_withSharpenedBrightened.keras')

"""### Model Training"""

from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint
epochs=50

# EarlyStopping will stop training if validation loss doesn't improve
early_stopping = EarlyStopping(
    monitor='val_loss', patience=5, verbose=1, restore_best_weights=True)

# Train your model
history = model_1.fit(
    dataset,
    validation_data=val_ds,
    epochs=epochs,
    callbacks=[early_stopping]
)

# Find the index where validation loss is minimized
best_epoch = history.history['val_loss'].index(min(history.history['val_loss']))

# Number of epochs trained before early stopping
epochs_trained = best_epoch + 1
print("Number of epochs trained:", epochs_trained)

"""#### Use only if need to train model 2 times (when dataset too huge)

##### Part 1
"""

from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint
epochs=50

# EarlyStopping will stop training if validation loss doesn't improve
early_stopping = EarlyStopping(
    monitor='val_loss', patience=5, verbose=1, restore_best_weights=True)

checkpoint = ModelCheckpoint('/content/drive/MyDrive/2023_internship/dataset_20200803/model0_phase1.h5', save_best_only=True)

# Train your model
history = model_1.fit(
    train_ds_part1,
    validation_data=val_ds,
    epochs=epochs,
    callbacks=[early_stopping, checkpoint]
)

# Find the index where validation loss is minimized
best_epoch = history.history['val_loss'].index(min(history.history['val_loss']))

# Number of epochs trained before early stopping
epochs_trained = best_epoch + 1
print("Number of epochs trained:", epochs_trained)

"""##### Part 2"""

# Load the saved model's weights from the first phase
model_1.load_weights('/content/drive/MyDrive/2023_internship/dataset_20200803/model0_phase1.h5')

checkpoint = ModelCheckpoint('/content/drive/MyDrive/2023_internship/dataset_20200803/model0_phase2.h5', save_best_only=True)

# Continue training the model on the remaining part of the dataset
history_phase2 = model_1.fit(
    train_ds_part2,
    validation_data=val_ds,
    epochs=epochs,
    callbacks=[early_stopping, checkpoint]
)

# Find the index where validation loss is minimized
best_epoch = history.history['val_loss'].index(min(history.history['val_loss']))

# Number of epochs trained before early stopping
epochs_trained = best_epoch + 1
print("Number of epochs trained:", epochs_trained)

"""### Save the Model"""

model_1.save('/content/drive/MyDrive/flirModel_withSharpenedBrightened_onlyFlipped.keras')